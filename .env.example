# Model settings
MODEL_PATH=mlx-community/Llama-3.2-3B-Instruct-4bit
ADAPTER_PATH=adapters

# Training settings
BATCH_SIZE=4
LEARNING_RATE=2e-4
NUM_LAYERS=1
GRAD_CHECKPOINT=true

# Validation settings
STEPS_PER_EVAL=total_iters
VAL_BATCHES=2
STEPS_PER_REPORT=10
SAVE_EVERY=total_iters

# Data settings
DATA_DIR=data
MAX_SEQ_LENGTH=2048

# Note: LoRA parameters are handled internally by MLX-LM
# Note: total_iters = ceil(num_examples / batch_size) * epochs
