# Model settings
MODEL_PATH=mlx-community/Hermes-2-Pro-Llama-3-8B-4bit
ADAPTER_PATH=adapters

# Training settings
BATCH_SIZE=4
LEARNING_RATE=1e-4
NUM_ITERS=1000
NUM_LAYERS=16
GRAD_CHECKPOINT=false

# LoRA settings
LORA_RANK=8
LORA_ALPHA=16
LORA_DROPOUT=0.05

# Output settings
OUTPUT_DIR=outputs
SAVE_EVERY=100